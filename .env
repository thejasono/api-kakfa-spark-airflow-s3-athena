###############################################################################
# AIRFLOW / POSTGRES
###############################################################################
# Airflow credentials
AIRFLOW_ADMIN_USER=admin
AIRFLOW_ADMIN_PASSWORD=admin
AIRFLOW_ADMIN_EMAIL=admin@example.com
AIRFLOW_ADMIN_FN=Admin
AIRFLOW_ADMIN_LN=User
# Postgres credentials for Airflow metadata DB (created on first run).
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow_db

# Airflow Fernet key: used to encrypt connections/variables.
# Generate once (keep it secret):
#   python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
AIRFLOW_FERNET_KEY=

# Airflow Webserver secret key: signs session cookies and log fetch requests.
# Generate a long random string once (example below is placeholder-quality only).
#   python -c "import secrets; print(secrets.token_urlsafe(64))"
AIRFLOW_SECRET_KEY=


# Container timezone (logs, schedulers, Spark UI etc.)
TZ=Europe/London


###############################################################################
# KAFKA (KRaft mode â€” NO ZooKeeper)
###############################################################################
# Cluster ID is REQUIRED in KRaft. Generate a Base64 UUID once:
#   docker run --rm confluentinc/cp-kafka:7.6.1 kafka-storage random-uuid
# Use the same value across all Kafka nodes (if you scale later).

# Hostname/IP your *host* clients use to reach Kafka (EXTERNAL listener).
# Usually "localhost". If connecting from another machine, put your LAN IP/DNS.
DOCKER_HOST_IP=localhost


###############################################################################
# KAFKA CONNECT (managed connectors)
###############################################################################
# Connect talks to Kafka via INTERNAL listener (inside Docker).
CONNECT_BOOTSTRAP_SERVERS=kafka:19092

# REST API port for managing connectors.
CONNECT_REST_PORT=8083

# Internal housekeeping topics and consumer group for Connect itself.
# For a single-node dev cluster these names are fine.
CONNECT_GROUP_ID=connect-cluster
CONNECT_CONFIG_STORAGE_TOPIC=_connect-configs
CONNECT_OFFSET_STORAGE_TOPIC=_connect-offsets
CONNECT_STATUS_STORAGE_TOPIC=_connect-status

# Converters: JSON is dead-simple for dev. Disable schemas to avoid the
# {"schema":...,"payload":...} wrapper (schemaless JSON is easier to eyeball).
CONNECT_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
CONNECT_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE=false
CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE=false
CONNECT_INTERNAL_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
CONNECT_INTERNAL_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter

# Where Connect looks for connector plugins (leave as is unless you add custom jars).
CONNECT_PLUGIN_PATH=/usr/share/java,/etc/kafka-connect/jars

# Tame the logs a bit.
CONNECT_LOG4J_ROOT_LOGLEVEL=INFO
CONNECT_LOG4J_LOGGERS=org.reflections=ERROR


###############################################################################
# SCHEMA REGISTRY (optional but useful if you move to Avro/Protobuf)
###############################################################################
# With KRaft, SR talks directly to Kafka via bootstrap servers.
SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS=PLAINTEXT://kafka:19092
SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL=PLAINTEXT
SCHEMA_REGISTRY_HOST_NAME=kafka_schema_registry
SCHEMA_REGISTRY_LISTENERS=http://0.0.0.0:8081


###############################################################################
# KAFKA UI (Provectus)
###############################################################################
# Define a single visible cluster in the UI using INTERNAL listener.
KAFKA_CLUSTERS_0_NAME=dev-cluster
KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:19092
KAFKA_CLUSTERS_0_SCHEMAREGISTRY=http://kafka_schema_registry:8081
KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME=connect
KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS=http://kafka_connect:8083
KAFKA_CLUSTER_ID=
# Allow editing clusters in the UI at runtime (handy in dev).
DYNAMIC_CONFIG_ENABLED=true


###############################################################################
# SPARK (Master)
###############################################################################
# These match the compose; included here for visibility.
SPARK_MODE=master
SPARK_RPC_AUTHENTICATION_ENABLED=no
SPARK_RPC_ENCRYPTION_ENABLED=no
# Spark UI inside the container is 8080; compose maps it to host 8085.
# (No need to set SPARK_UI_PORT unless you change the container port)


###############################################################################
# MINIO (local S3-compatible object storage)
###############################################################################
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin
MINIO_SERVER_PORT=9000
MINIO_CONSOLE_PORT=9001
# The bootstrap helper container now derives its bucket target from S3_BUCKET,
# so keep S3_BUCKET updated instead of editing a separate MINIO_BUCKET value.


###############################################################################
# SHARED OBJECT STORAGE SETTINGS (used by Spark streaming job)
###############################################################################

S3_BUCKET=namegeneratorbucket
S3_OUTPUT_PREFIX=names
S3_CHECKPOINT_PREFIX=checkpoints/names
S3_REGION=eu-west-2
# Leave blank to use the standard AWS endpoint derived from the region.
S3_ENDPOINT=https://s3.eu-west-2.amazonaws.com
# Set to "true" only when targeting gateways that require path-style access.
S3_PATH_STYLE_ACCESS=


###############################################################################
# LOCAL USER MAPPING FOR AIRFLOW CONTAINERS
###############################################################################
# Match the container user/group with the host to avoid permission errors.
# Update AIRFLOW_UID if running as a non-root user (e.g. run: echo "AIRFLOW_UID=$(id -u)" >> .env).
AIRFLOW_UID=50000
AIRFLOW_GID=0


# Spark inherits AWS credentials from the container environment. Provide
# long-lived access keys here only if you are not relying on the default AWS
# credential chain (for example, when running on a standalone Docker host
# without IAM roles). Leave these blank to defer to `~/.aws` profiles or
# instance metadata.

AWS_REGION=eu-west-2
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_SESSION_TOKEN=
AWS_CREDENTIAL_EXPIRATION=
